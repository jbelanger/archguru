===== ../src/archguru/core/config.py =====
"""
Configuration management for ArchGuru
Handles OpenRouter API keys and model settings
"""
import os
from typing import List, Dict, Any
from dotenv import load_dotenv

load_dotenv()


class Config:
    """Configuration settings for the ArchGuru platform"""

    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
    OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"

    # Model teams for competition (configurable via environment variables)
    DEFAULT_MODEL_TEAMS = {
        "team_a": [
            "openai/gpt-oss-120b",
            "openai/gpt-5-nano"
        ],
        "team_b": [
            "qwen/qwen3-next-80b-a3b-thinking",
            "qwen/qwen3-next-80b-a3b-instruct"
        ],
        "team_c": [
            "openrouter/sonoma-dusk-alpha",
            "openrouter/sonoma-sky-alpha"
        ]
    }

    @classmethod
    def get_model_teams(cls) -> Dict[str, List[str]]:
        """Get model teams from environment variables or defaults"""
        model_teams = {}

        # Load custom model teams from environment
        team_a_models = os.getenv("ARCHGURU_TEAM_A_MODELS", "").split(",")
        team_b_models = os.getenv("ARCHGURU_TEAM_B_MODELS", "").split(",")
        team_c_models = os.getenv("ARCHGURU_TEAM_C_MODELS", "").split(",")

        # Use environment variables if provided, otherwise use defaults
        model_teams["team_a"] = [m.strip() for m in team_a_models if m.strip()] or cls.DEFAULT_MODEL_TEAMS["team_a"]
        model_teams["team_b"] = [m.strip() for m in team_b_models if m.strip()] or cls.DEFAULT_MODEL_TEAMS["team_b"]
        model_teams["team_c"] = [m.strip() for m in team_c_models if m.strip()] or cls.DEFAULT_MODEL_TEAMS["team_c"]

        return model_teams

    @classmethod
    def get_arbiter_model(cls) -> str:
        """Get the final arbiter model from environment variables or default"""
        return os.getenv("ARCHGURU_ARBITER_MODEL", "openai/gpt-4o")

    # Decision types supported
    DECISION_TYPES = [
        "project-structure",
        "database",
        "deployment",
        "api-design"
    ]

    @classmethod
    def validate(cls) -> bool:
        """Validate that required configuration is present"""
        if not cls.OPENROUTER_API_KEY:
            print("❌ Error: OPENROUTER_API_KEY not found in environment")
            print("   Please create a .env file with your OpenRouter API key")
            return False
        return True===== ../src/archguru/core/__init__.py =====
===== ../src/archguru/core/state.py =====
"""
LangGraph state management for model competition pipeline
"""
from typing import List, Optional, TypedDict
from ..models.decision import DecisionRequest, ModelResponse, DecisionResult


class CompetitionState(TypedDict):
    """State for the model competition pipeline"""
    request: DecisionRequest
    model_responses: List[ModelResponse]
    winning_model: Optional[str]
    consensus_recommendation: Optional[str]
    error_message: Optional[str]
    current_step: str===== ../src/archguru/__init__.py =====
===== ../src/archguru/agents/__init__.py =====
===== ../src/archguru/agents/debate.py =====
"""
Cross-model debate and evaluation logic for ArchGuru
Handles model-vs-model competition and consensus building
"""
import asyncio
from typing import List, Dict, Any, Optional
from ..models.decision import ModelResponse
from ..api.openrouter import OpenRouterClient
from ..core.config import Config


class ModelDebateEngine:
    """Manages cross-model debates and evaluation"""

    def __init__(self):
        self.client = OpenRouterClient()

    async def run_cross_model_debate(self, responses: List[ModelResponse]) -> Dict[str, Any]:
        """
        Run cross-model debate where models critique each other's recommendations
        """
        if len(responses) < 2:
            return {
                "debate_summary": "Not enough responses for debate",
                "winning_model": responses[0].model_name if responses else None,
                "consensus_recommendation": responses[0].recommendation if responses else None
            }

        print("🥊 Starting cross-model debate...")

        # Prepare debate context
        debate_context = self._prepare_debate_context(responses)

        # Run arbiter evaluation
        arbiter_result = await self._run_arbiter_evaluation(debate_context, responses)

        return arbiter_result

    def _prepare_debate_context(self, responses: List[ModelResponse]) -> str:
        """Prepare context for the debate with all model responses"""
        context = "ARCHITECTURAL DECISION RESPONSES FROM COMPETING AI MODELS:\n\n"

        for i, response in enumerate(responses, 1):
            context += f"=== MODEL {i}: {response.model_name} ===\n"
            context += f"Recommendation: {response.recommendation}\n"
            context += f"Reasoning: {response.reasoning}\n"
            context += f"Research Steps: {len(response.research_steps)} tool calls\n"
            context += f"Response Time: {response.response_time:.2f}s\n"

            if response.trade_offs:
                context += f"Trade-offs: {', '.join(response.trade_offs)}\n"

            context += "\n" + "="*50 + "\n\n"

        return context

    async def _run_arbiter_evaluation(self, debate_context: str, responses: List[ModelResponse]) -> Dict[str, Any]:
        """Run final arbiter model to evaluate all responses and pick winner"""
        arbiter_model = Config.get_arbiter_model()

        arbiter_prompt = f"""You are the final arbiter in an AI architecture competition. Below are responses from multiple AI models to the same architectural decision request.

{debate_context}

Your task is to:
1. Analyze each model's recommendation for technical accuracy, completeness, and practicality
2. Evaluate the quality of research each model performed
3. Consider which recommendation would work best in production
4. Select the winning model and explain why
5. Synthesize the best ideas into a final consensus recommendation

Provide your evaluation in this format:

WINNER: [model_name]

EVALUATION:
[Your detailed analysis of why this model won, referencing specific strengths and addressing weaknesses of other responses]

CONSENSUS RECOMMENDATION:
[Your synthesized recommendation that combines the best aspects of all responses]

DEBATE SUMMARY:
[Brief summary of the key differences between models and what made the winner stand out]"""

        print(f"  🏅 Arbiter evaluation using {arbiter_model}...")

        try:
            # Get arbiter's evaluation (without research tools for final judgment)
            arbiter_response = await self._get_arbiter_response(arbiter_model, arbiter_prompt)

            # Parse arbiter response
            winner, evaluation, consensus, summary = self._parse_arbiter_response(arbiter_response)

            return {
                "winning_model": winner,
                "debate_summary": summary,
                "consensus_recommendation": consensus,
                "arbiter_evaluation": evaluation,
                "arbiter_model": arbiter_model
            }

        except Exception as e:
            print(f"❌ Arbiter evaluation failed: {str(e)}")
            # Fallback to simple scoring
            return self._fallback_evaluation(responses)

    async def _get_arbiter_response(self, model_name: str, prompt: str) -> str:
        """Get response from arbiter model without research tools"""
        try:
            response = self.client.client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.3  # Lower temperature for more consistent evaluation
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"Arbiter model {model_name} failed: {str(e)}")

    def _parse_arbiter_response(self, response: str) -> tuple:
        """Parse the arbiter's structured response"""
        lines = response.split('\n')
        winner = None
        evaluation = ""
        consensus = ""
        summary = ""

        current_section = None

        for line in lines:
            line = line.strip()

            if line.startswith("WINNER:"):
                winner = line.replace("WINNER:", "").strip()
            elif line.startswith("EVALUATION:"):
                current_section = "evaluation"
                continue
            elif line.startswith("CONSENSUS RECOMMENDATION:"):
                current_section = "consensus"
                continue
            elif line.startswith("DEBATE SUMMARY:"):
                current_section = "summary"
                continue
            elif line and current_section:
                if current_section == "evaluation":
                    evaluation += line + "\n"
                elif current_section == "consensus":
                    consensus += line + "\n"
                elif current_section == "summary":
                    summary += line + "\n"

        return (
            winner or "No winner selected",
            evaluation.strip() or "No evaluation provided",
            consensus.strip() or "No consensus reached",
            summary.strip() or "No summary available"
        )

    def _fallback_evaluation(self, responses: List[ModelResponse]) -> Dict[str, Any]:
        """Fallback evaluation based on simple metrics when arbiter fails"""
        if not responses:
            return {
                "winning_model": None,
                "debate_summary": "No responses to evaluate",
                "consensus_recommendation": "No recommendations available"
            }

        # Simple scoring: research steps + response quality + confidence
        best_response = None
        best_score = -1

        for response in responses:
            if response.recommendation.startswith("Error:"):
                continue

            score = (
                len(response.research_steps) * 2 +  # Research effort
                len(response.reasoning) / 50 +       # Reasoning depth
                response.confidence_score * 10       # Model confidence
            )

            if score > best_score:
                best_score = score
                best_response = response

        if best_response:
            return {
                "winning_model": best_response.model_name,
                "debate_summary": f"Winner selected by fallback scoring (score: {best_score:.1f})",
                "consensus_recommendation": best_response.recommendation,
                "arbiter_evaluation": "Fallback evaluation used due to arbiter failure"
            }
        else:
            return {
                "winning_model": responses[0].model_name,
                "debate_summary": "All models failed, selected first response",
                "consensus_recommendation": responses[0].recommendation
            }===== ../src/archguru/agents/pipeline.py =====
"""
LangGraph pipeline for model team competition and debate
Phase 2: Multi-model competition with cross-model debate
"""
import asyncio
from typing import Dict, Any
from langgraph.graph import StateGraph, END
from ..core.state import CompetitionState
from ..models.decision import DecisionRequest, DecisionResult
from ..api.openrouter import OpenRouterClient
from .debate import ModelDebateEngine


class ModelCompetitionPipeline:
    """LangGraph pipeline for multi-model team competition"""

    def __init__(self):
        self.client = OpenRouterClient()
        self.debate_engine = ModelDebateEngine()
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """Build the LangGraph competition pipeline"""
        workflow = StateGraph(CompetitionState)

        # Phase 2: Multi-model competition
        workflow.add_node("run_model_competition", self._run_model_competition)
        workflow.add_node("run_debate", self._run_debate)
        workflow.add_node("generate_final_result", self._generate_final_result)

        # Set entry point
        workflow.set_entry_point("run_model_competition")

        # Add edges
        workflow.add_edge("run_model_competition", "run_debate")
        workflow.add_edge("run_debate", "generate_final_result")
        workflow.add_edge("generate_final_result", END)

        return workflow.compile()


    async def _run_model_competition(self, state: CompetitionState) -> Dict[str, Any]:
        """Run multi-model team competition"""
        request = state["request"]

        try:
            responses = await self.client.run_model_team_competition(
                decision_type=request.decision_type,
                language=request.language,
                framework=request.framework,
                requirements=request.requirements
            )

            return {
                "model_responses": responses,
                "current_step": "competition_complete"
            }

        except Exception as e:
            return {
                "error_message": f"Model competition failed: {str(e)}",
                "current_step": "error"
            }

    async def _run_debate(self, state: CompetitionState) -> Dict[str, Any]:
        """Run cross-model debate and evaluation"""
        responses = state["model_responses"]

        if not responses:
            return {
                "error_message": "No model responses to debate",
                "current_step": "error"
            }

        try:
            debate_result = await self.debate_engine.run_cross_model_debate(responses)

            return {
                "winning_model": debate_result["winning_model"],
                "consensus_recommendation": debate_result["consensus_recommendation"],
                "debate_summary": debate_result["debate_summary"],
                "arbiter_evaluation": debate_result.get("arbiter_evaluation"),
                "current_step": "debate_complete"
            }

        except Exception as e:
            return {
                "error_message": f"Debate failed: {str(e)}",
                "current_step": "error"
            }

    async def _generate_final_result(self, state: CompetitionState) -> Dict[str, Any]:
        """Generate final competition result"""
        responses = state["model_responses"]
        winning_model = state.get("winning_model")
        consensus = state.get("consensus_recommendation")

        if responses and winning_model and consensus:
            successful_models = [r for r in responses if not r.recommendation.startswith("Error:")]
            return {
                "consensus_recommendation": consensus,
                "debate_summary": f"Competition complete: {len(successful_models)}/{len(responses)} models succeeded. Winner: {winning_model}",
                "current_step": "complete"
            }
        else:
            return {
                "consensus_recommendation": "Competition failed to produce results",
                "debate_summary": "Model competition encountered errors",
                "current_step": "complete"
            }

    async def run(self, request: DecisionRequest) -> DecisionResult:
        """Run the complete competition pipeline"""
        initial_state: CompetitionState = {
            "request": request,
            "model_responses": [],
            "winning_model": None,
            "consensus_recommendation": None,
            "error_message": None,
            "current_step": "starting"
        }

        print("🔄 Starting LangGraph model competition pipeline...")
        result = await self.graph.ainvoke(initial_state)

        return DecisionResult(
            request=request,
            model_responses=result.get("model_responses", []),
            winning_model=result.get("winning_model"),
            consensus_recommendation=result.get("consensus_recommendation"),
            debate_summary=result.get("debate_summary", "Competition complete")
        )===== ../src/archguru/models/__init__.py =====
===== ../src/archguru/models/decision.py =====
"""
Data models for architecture decisions and model responses
"""
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from datetime import datetime


@dataclass
class DecisionRequest:
    """Represents an architecture decision request"""
    decision_type: str
    language: Optional[str] = None
    framework: Optional[str] = None
    requirements: Optional[str] = None
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


@dataclass
class ModelResponse:
    """Response from a single model with research tracking"""
    model_name: str
    team: str  # openai, claude, llama
    recommendation: str
    reasoning: str
    trade_offs: List[str]
    confidence_score: float
    response_time: float
    research_steps: List[Dict[str, Any]] = None
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()
        if self.research_steps is None:
            self.research_steps = []


@dataclass
class DecisionResult:
    """Final result with all model responses and competition outcome"""
    request: DecisionRequest
    model_responses: List[ModelResponse]
    winning_model: Optional[str] = None
    consensus_recommendation: Optional[str] = None
    debate_summary: Optional[str] = None
    total_time: Optional[float] = None===== ../src/archguru/cli/__init__.py =====
===== ../src/archguru/cli/main.py =====
#!/usr/bin/env python3
"""
ArchGuru CLI - Universal AI Architecture Decision Platform
Phase 2: Multi-model team competition with cross-model debate
"""
import argparse
import asyncio
import sys

from ..models.decision import DecisionRequest
from ..agents.pipeline import ModelCompetitionPipeline
from ..core.config import Config


def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="archguru",
        description="Universal AI Architecture Decision Platform - Get architectural guidance from competing AI models"
    )

    parser.add_argument(
        "--type",
        choices=["project-structure", "database", "deployment", "api-design"],
        required=True,
        help="Type of architectural decision to make"
    )

    parser.add_argument(
        "--language",
        help="Programming language or technology stack"
    )

    parser.add_argument(
        "--framework",
        help="Framework or specific technology preference"
    )

    parser.add_argument(
        "--requirements",
        help="Additional requirements or constraints"
    )

    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Enable verbose output including model details and arbiter evaluation"
    )

    return parser


async def run_decision(args) -> int:
    """Run the architectural decision process"""
    print("🏗️  ArchGuru Phase 2 - AI Architecture Model Competition Platform")
    print("=" * 70)

    print(f"Decision Type: {args.type}")
    if args.language:
        print(f"Language/Stack: {args.language}")
    if args.framework:
        print(f"Framework: {args.framework}")
    if args.requirements:
        print(f"Requirements: {args.requirements}")

    if not Config.validate():
        print("\n❌ Configuration error - please check your .env file")
        return 1

    request = DecisionRequest(
        decision_type=args.type,
        language=args.language,
        framework=args.framework,
        requirements=args.requirements
    )

    try:
        pipeline = ModelCompetitionPipeline()
        result = await pipeline.run(request)

        await _display_competition_results(result, args.verbose)
        return 0

    except Exception as e:
        print(f"\n❌ Error: {str(e)}")
        if args.verbose:
            import traceback
            print(f"\nFull traceback:\n{traceback.format_exc()}")
        return 1


async def _display_competition_results(result, verbose: bool = False):
    """Display results from model competition"""
    def _preview(text: str, lines: int = 12) -> str:
        return "\n".join((text or "").strip().splitlines()[:lines])

    responses = result.model_responses or []
    successful_responses = [r for r in responses if not r.recommendation.startswith("Error:")]

    print(f"\n🏆 Competition Results:")
    print(f"Models competed: {len(responses)}")
    print(f"Successful responses: {len(successful_responses)}")

    if result.winning_model:
        print(f"🥇 Winner: {result.winning_model}")

    print(f"\n📋 Final Recommendation:")
    print(result.consensus_recommendation or "No consensus reached")

    if verbose:
        # v0.2 requirement: side-by-side recommendations + research approach
        print(f"\n🧪 Side-by-side recommendations:")
        for r in responses:
            if r.recommendation.startswith("Error:"):
                continue
            funcs = [step.get("function", "") for step in (r.research_steps or [])]
            print(f"\n--- {r.model_name} ({r.team}) — {r.response_time:.2f}s ---")
            print(_preview(r.recommendation, 12))
            if funcs:
                print(f"🔎 Research approach: {', '.join(funcs)}")

        print(f"\n🎯 Individual Model Performance:")
        for i, response in enumerate(responses, 1):
            status = "✅" if not response.recommendation.startswith("Error:") else "❌"
            print(f"  {i}. {status} {response.model_name} ({response.team})")
            print(f"     Research: {len(response.research_steps)} steps")
            print(f"     Time: {response.response_time:.2f}s")
            if response.recommendation.startswith("Error:"):
                print(f"     Error: {response.recommendation}")
            print()

        if result.debate_summary:
            print(f"🥊 Debate Summary:")
            print(result.debate_summary)




def main():
    """Main CLI entry point"""
    parser = create_parser()
    args = parser.parse_args()

    return asyncio.run(run_decision(args))


if __name__ == "__main__":
    sys.exit(main())===== ../src/archguru/api/openrouter.py =====
"""
OpenRouter API integration with function calling support
Handles LLM requests with research tool access
"""
import asyncio
import json
from typing import List, Dict, Any
from openai import OpenAI
from ..core.config import Config
from ..models.decision import ModelResponse
from .github import GitHubClient
from .reddit import RedditClient
from .stackoverflow import StackOverflowClient
import time


class OpenRouterClient:
    """Client for interacting with OpenRouter API with research tools"""

    def __init__(self):
        if not Config.validate():
            raise ValueError("Invalid configuration")

        self.client = OpenAI(
            base_url=Config.OPENROUTER_BASE_URL,
            api_key=Config.OPENROUTER_API_KEY
        )

        # Initialize research tools
        self.github = GitHubClient()
        self.reddit = RedditClient()
        self.stackoverflow = StackOverflowClient()

    def get_research_tools(self) -> List[Dict[str, Any]]:
        """Define available research tools for the LLM"""
        return [
            {
                "type": "function",
                "function": {
                    "name": "search_github_repos",
                    "description": "Search GitHub repositories for examples and patterns",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "Search query"},
                            "language": {"type": "string", "description": "Programming language filter"},
                            "limit": {"type": "integer", "description": "Number of results (max 10)", "default": 5}
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_reddit_discussions",
                    "description": "Search Reddit for community discussions and opinions",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "Search query"},
                            "subreddits": {"type": "array", "items": {"type": "string"}, "description": "Specific subreddits to search"},
                            "limit": {"type": "integer", "description": "Number of results (max 10)", "default": 5}
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_stackoverflow",
                    "description": "Search StackOverflow for technical questions and solutions",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "Search query"},
                            "tags": {"type": "array", "items": {"type": "string"}, "description": "Technology tags to filter by"},
                            "limit": {"type": "integer", "description": "Number of results (max 10)", "default": 5}
                        },
                        "required": ["query"]
                    }
                }
            }
        ]

    def execute_tool_call(self, tool_call) -> str:
        """Execute a tool call and return results"""
        function_name = tool_call.function.name
        arguments = json.loads(tool_call.function.arguments)

        try:
            if function_name == "search_github_repos":
                results = self.github.search_repositories(**arguments)
                return json.dumps(results, indent=2)

            elif function_name == "search_reddit_discussions":
                results = self.reddit.search_discussions(**arguments)
                return json.dumps(results, indent=2)

            elif function_name == "search_stackoverflow":
                results = self.stackoverflow.search_questions(**arguments)
                return json.dumps(results, indent=2)

            else:
                return f"Unknown function: {function_name}"

        except Exception as e:
            return f"Error executing {function_name}: {str(e)}"

    async def get_model_response(self, model_name: str, prompt: str) -> ModelResponse:
        """Get response from a single model with research tools"""
        start_time = time.time()
        research_steps = []

        try:
            messages = [{"role": "user", "content": prompt}]
            tools = self.get_research_tools()

            # Try initial request with tools
            try:
                response = self.client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    tools=tools,
                    max_tokens=2000,
                    temperature=0.7
                )
            except Exception as tool_error:
                # If tools fail (404/400), fallback to simple completion
                if "tool" in str(tool_error).lower() or "404" in str(tool_error) or "400" in str(tool_error):
                    print(f"  ⚠️  {model_name} doesn't support tools, using basic completion")
                    response = self.client.chat.completions.create(
                        model=model_name,
                        messages=messages,
                        max_tokens=2000,
                        temperature=0.7
                    )
                    # Return basic response without research
                    response_time = time.time() - start_time
                    content = response.choices[0].message.content
                    parts = content.split('\n\n', 2)
                    recommendation = parts[0] if parts else content[:200]
                    reasoning = parts[1] if len(parts) > 1 else "Basic response without research"

                    return ModelResponse(
                        model_name=model_name,
                        team="basic",
                        recommendation=recommendation,
                        reasoning=reasoning,
                        trade_offs=["No research performed"],
                        confidence_score=0.6,
                        response_time=response_time,
                        research_steps=[]
                    )
                else:
                    raise tool_error

            # Handle tool calls
            while response.choices[0].message.tool_calls:
                assistant_message = response.choices[0].message
                messages.append(assistant_message)

                for tool_call in assistant_message.tool_calls:
                    print(f"  🔍 {model_name} researching: {tool_call.function.name}")
                    research_steps.append({
                        "function": tool_call.function.name,
                        "arguments": tool_call.function.arguments
                    })

                    tool_result = self.execute_tool_call(tool_call)
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": tool_result
                    })

                # Get next response
                response = self.client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    tools=tools,
                    max_tokens=2000,
                    temperature=0.7
                )

            response_time = time.time() - start_time
            content = response.choices[0].message.content

            # Parse the final response
            parts = content.split('\n\n', 2)
            recommendation = parts[0] if parts else content[:200]
            reasoning = parts[1] if len(parts) > 1 else "See full response"

            return ModelResponse(
                model_name=model_name,
                team="research",
                recommendation=recommendation,
                reasoning=reasoning,
                trade_offs=["Analysis based on research"],
                confidence_score=0.8,
                response_time=response_time,
                research_steps=research_steps
            )

        except Exception as e:
            print(f"❌ Error with {model_name}: {str(e)}")
            return ModelResponse(
                model_name=model_name,
                team="research",
                recommendation=f"Error: {str(e)}",
                reasoning="Model failed to respond",
                trade_offs=[],
                confidence_score=0.0,
                response_time=time.time() - start_time,
                research_steps=research_steps
            )


    async def run_model_team_competition(self, decision_type: str, language: str = None,
                                       framework: str = None, requirements: str = None) -> List[ModelResponse]:
        """Run multi-model team competition for Phase 2"""
        prompt = f"""You are an expert software architect competing with other AI models to provide the best architectural guidance. I need your help with an architectural decision.

Decision Type: {decision_type}
Language/Stack: {language or 'Not specified'}
Framework: {framework or 'Not specified'}
Requirements: {requirements or 'None specified'}

Before making your recommendation, please research this topic using the available tools:
- Search GitHub for examples and patterns
- Look at community discussions on Reddit
- Check StackOverflow for technical considerations

Your response will be compared against other AI models, so provide:
1. Your specific recommendation with clear justification
2. Detailed reasoning based on your research findings
3. Trade-offs and alternatives you considered
4. Implementation considerations and best practices
5. Why your approach is superior to alternatives

Focus on practical, production-ready advice based on real-world evidence. Be confident and specific in your recommendations."""

        model_teams = Config.get_model_teams()
        responses = []

        print("🏆 Starting model team competition...")

        # Run all models concurrently for better performance
        tasks = []
        for team_name, models in model_teams.items():
            for model_name in models:
                print(f"  🤖 {team_name.upper()}: {model_name}")
                task = self.get_model_response(model_name, prompt)
                tasks.append(task)

        # Execute all models concurrently
        model_responses = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results and handle any exceptions
        model_index = 0
        for team_name, models in model_teams.items():
            for model_name in models:
                if model_index < len(model_responses):
                    response = model_responses[model_index]
                    if isinstance(response, Exception):
                        # Create error response for failed models
                        error_response = ModelResponse(
                            model_name=model_name,
                            team=team_name,
                            recommendation=f"Error: {str(response)}",
                            reasoning="Model failed to respond",
                            trade_offs=[],
                            confidence_score=0.0,
                            response_time=0.0,
                            research_steps=[]
                        )
                        responses.append(error_response)
                    else:
                        # Update team name for successful responses
                        response.team = team_name
                        responses.append(response)
                model_index += 1

        return responses===== ../src/archguru/api/__init__.py =====
===== ../src/archguru/api/stackoverflow.py =====
"""
StackOverflow API client for technical research
"""
import requests
from typing import List, Dict, Any
from ..core.config import Config


class StackOverflowClient:
    """Client for StackOverflow API research"""

    def __init__(self):
        self.base_url = "https://api.stackexchange.com/2.3"
        self.session = requests.Session()

    def search_questions(self, query: str, tags: List[str] = None,
                        sort: str = "votes", limit: int = 5) -> List[Dict[str, Any]]:
        """Search StackOverflow questions"""
        params = {
            "order": "desc",
            "sort": sort,
            "intitle": query,
            "site": "stackoverflow",
            "pagesize": limit
        }

        if tags:
            params["tagged"] = ";".join(tags)

        try:
            response = self.session.get(f"{self.base_url}/search", params=params, timeout=8)
            response.raise_for_status()

            results = []
            data = response.json()
            for question in data.get("items", []):
                results.append({
                    "title": question.get("title", ""),
                    "score": question.get("score", 0),
                    "view_count": question.get("view_count", 0),
                    "answer_count": question.get("answer_count", 0),
                    "tags": question.get("tags", []),
                    "url": question.get("link", ""),
                    "is_answered": question.get("is_answered", False)
                })

            return results

        except Exception as e:
            print(f"StackOverflow search error: {e}")
            return []

    def get_popular_tags(self, related_to: str = None, limit: int = 10) -> List[Dict[str, Any]]:
        """Get popular tags, optionally filtered"""
        params = {
            "order": "desc",
            "sort": "popular",
            "site": "stackoverflow",
            "pagesize": limit
        }

        if related_to:
            params["inname"] = related_to

        try:
            response = self.session.get(f"{self.base_url}/tags", params=params, timeout=8)
            response.raise_for_status()

            results = []
            data = response.json()
            for tag in data.get("items", []):
                results.append({
                    "name": tag.get("name", ""),
                    "count": tag.get("count", 0)
                })

            return results

        except Exception as e:
            print(f"StackOverflow tags error: {e}")
            return []===== ../src/archguru/api/github.py =====
"""
GitHub API client for repository research
"""
import requests
from typing import List, Dict, Any, Optional
from ..core.config import Config


class GitHubClient:
    """Client for GitHub API research"""

    def __init__(self):
        self.base_url = "https://api.github.com"
        self.session = requests.Session()
        # GitHub API doesn't require auth for basic searches, but rate limited

    def search_repositories(self, query: str, language: str = None,
                          sort: str = "stars", limit: int = 5) -> List[Dict[str, Any]]:
        """Search GitHub repositories"""
        search_query = query
        if language:
            search_query += f" language:{language}"

        params = {
            "q": search_query,
            "sort": sort,
            "order": "desc",
            "per_page": limit
        }

        try:
            response = self.session.get(f"{self.base_url}/search/repositories", params=params, timeout=8)
            response.raise_for_status()

            results = []
            for repo in response.json().get("items", []):
                results.append({
                    "name": repo["full_name"],
                    "description": repo.get("description", ""),
                    "stars": repo["stargazers_count"],
                    "language": repo.get("language"),
                    "url": repo["html_url"],
                    "topics": repo.get("topics", [])
                })
            return results

        except Exception as e:
            print(f"GitHub search error: {e}")
            return []

    def get_repository_structure(self, owner: str, repo: str) -> Dict[str, Any]:
        """Get repository file structure"""
        try:
            response = self.session.get(f"{self.base_url}/repos/{owner}/{repo}/contents", timeout=8)
            response.raise_for_status()

            structure = []
            for item in response.json():
                if item["type"] == "file":
                    structure.append({
                        "name": item["name"],
                        "type": "file",
                        "path": item["path"]
                    })
                elif item["type"] == "dir":
                    structure.append({
                        "name": item["name"],
                        "type": "directory",
                        "path": item["path"]
                    })

            return {"structure": structure}

        except Exception as e:
            print(f"GitHub repo structure error: {e}")
            return {"structure": []}===== ../src/archguru/api/reddit.py =====
"""
Reddit API client for community research
"""
import requests
from typing import List, Dict, Any
from ..core.config import Config


class RedditClient:
    """Client for Reddit community research"""

    def __init__(self):
        self.base_url = "https://www.reddit.com"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "archguru/1.0 (research bot)"
        })

    def search_discussions(self, query: str, subreddits: List[str] = None,
                          limit: int = 5) -> List[Dict[str, Any]]:
        """Search Reddit discussions"""
        if subreddits is None:
            subreddits = ["programming", "webdev", "Python", "javascript", "reactjs"]

        results = []

        for subreddit_raw in subreddits[:3]:  # Limit to avoid rate limits
            try:
                # Clean subreddit name - remove any r/ prefix and extra slashes
                subreddit = subreddit_raw.strip().replace("r/", "").replace("/", "")
                if not subreddit:
                    continue

                params = {
                    "q": query,
                    "limit": limit,
                    "sort": "top",
                    "t": "year"
                }

                response = self.session.get(
                    f"{self.base_url}/r/{subreddit}/search.json",
                    params=params,
                    timeout=8
                )
                response.raise_for_status()

                data = response.json()
                for post in data.get("data", {}).get("children", []):
                    post_data = post.get("data", {})
                    results.append({
                        "title": post_data.get("title", ""),
                        "subreddit": post_data.get("subreddit", ""),
                        "score": post_data.get("score", 0),
                        "num_comments": post_data.get("num_comments", 0),
                        "url": f"https://reddit.com{post_data.get('permalink', '')}",
                        "selftext": post_data.get("selftext", "")[:500]  # Truncate
                    })

            except Exception as e:
                print(f"Reddit search error for r/{subreddit}: {e}")
                continue

        return sorted(results, key=lambda x: x["score"], reverse=True)[:limit]

    def get_top_posts(self, subreddit: str, time_period: str = "month",
                      limit: int = 5) -> List[Dict[str, Any]]:
        """Get top posts from a subreddit"""
        try:
            params = {
                "limit": limit,
                "t": time_period
            }

            response = self.session.get(
                f"{self.base_url}/r/{subreddit}/top.json",
                params=params,
                timeout=8
            )
            response.raise_for_status()

            results = []
            data = response.json()
            for post in data.get("data", {}).get("children", []):
                post_data = post.get("data", {})
                results.append({
                    "title": post_data.get("title", ""),
                    "score": post_data.get("score", 0),
                    "num_comments": post_data.get("num_comments", 0),
                    "url": f"https://reddit.com{post_data.get('permalink', '')}",
                    "selftext": post_data.get("selftext", "")[:300]
                })

            return results

        except Exception as e:
            print(f"Reddit top posts error: {e}")
            return []