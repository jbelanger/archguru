# Part 4: Implementation Roadmap

## Status

**Proposed** (v1.0 - January 2025)  
Implementation plan for ADR-002, ADR-003, ADR-004

## Context

This roadmap defines how to build the adversarial collaboration system with tournament-style debate mechanics. Each phase delivers a working system with increasing sophistication.

## Implementation Phases

### Phase 1: Core Debate Engine (v0.8)

**Timeline:** 3-4 days  
**Goal:** Establish the fundamental debate mechanics between two models

#### System Architecture:

```
src/archguru/
  ├── debate/
  │   ├── __init__.py
  │   ├── match.py           # DebateMatch state machine
  │   ├── rounds.py          # Round orchestration
  │   ├── convergence.py     # Convergence measurement
  │   └── prompts.py         # Challenge/defense templates
  ├── models/
  │   ├── confidence.py      # Confidence scoring system
  │   └── response.py        # Structured response format
  └── storage/
      └── debate_schema.sql   # Database schema
```

#### Database Schema:

```sql
-- Core debate tables
CREATE TABLE debate_match (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  run_id UUID NOT NULL REFERENCES run(id),
  model_a_id INTEGER NOT NULL REFERENCES model(id),
  model_b_id INTEGER NOT NULL REFERENCES model(id),
  winner_model_id INTEGER REFERENCES model(id),

  -- Configuration
  max_rounds INTEGER DEFAULT 3,
  convergence_threshold REAL DEFAULT 80.0,

  -- Results
  final_convergence_score REAL,
  total_rounds_completed INTEGER,
  debate_status TEXT, -- 'converged' | 'forced' | 'failed'

  -- Tournament context
  match_number INTEGER DEFAULT 1,
  previous_match_id UUID REFERENCES debate_match(id),

  -- Metadata
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  completed_at TIMESTAMP,
  total_cost_usd REAL
);

CREATE TABLE debate_round (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  match_id UUID NOT NULL REFERENCES debate_match(id),
  round_number INTEGER NOT NULL,
  round_type TEXT NOT NULL, -- 'initial' | 'challenge' | 'defense'

  -- Model A response
  model_a_response TEXT NOT NULL,
  model_a_confidence REAL,
  model_a_confidence_factors JSONB,
  model_a_uncertainty_points JSONB,
  model_a_tool_calls JSONB,

  -- Model B response
  model_b_response TEXT NOT NULL,
  model_b_confidence REAL,
  model_b_confidence_factors JSONB,
  model_b_uncertainty_points JSONB,
  model_b_tool_calls JSONB,

  -- Convergence measurement
  convergence_score REAL,
  convergence_method TEXT, -- 'structural' | 'semantic' | 'arbiter'
  convergence_details JSONB,

  -- Position tracking
  position_changed_a BOOLEAN DEFAULT FALSE,
  position_changed_b BOOLEAN DEFAULT FALSE,

  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

  UNIQUE(match_id, round_number)
);

CREATE INDEX idx_debate_round_lookup ON debate_round(match_id, round_number);
CREATE INDEX idx_debate_match_run ON debate_match(run_id);
```

#### Core State Machine:

```python
# src/archguru/debate/match.py
from enum import Enum
from typing import Optional, List
import asyncio

class MatchState(Enum):
    INITIALIZED = "initialized"
    RESEARCHING = "researching"
    CHALLENGING = "challenging"
    DEFENDING = "defending"
    CHECKING_CONVERGENCE = "checking_convergence"
    COMPLETED = "completed"
    FAILED = "failed"

class DebateMatch:
    def __init__(
        self,
        model_a: str,
        model_b: str,
        context: DecisionContext,
        max_rounds: int = 3,
        convergence_threshold: float = 80.0,
        previous_matches: Optional[List['DebateMatch']] = None
    ):
        self.model_a = model_a
        self.model_b = model_b
        self.context = context
        self.max_rounds = max_rounds
        self.convergence_threshold = convergence_threshold
        self.previous_matches = previous_matches or []
        self.rounds: List[DebateRound] = []
        self.state = MatchState.INITIALIZED

    async def run(self) -> MatchResult:
        """Execute the complete debate match"""
        try:
            # Round 0: Initial research and positions
            self.state = MatchState.RESEARCHING
            initial_round = await self._run_initial_round()
            self.rounds.append(initial_round)

            # Debate cycles
            for cycle in range(self.max_rounds):
                # Challenge round
                self.state = MatchState.CHALLENGING
                challenge_round = await self._run_challenge_round(
                    round_num=cycle * 2 + 1,
                    previous_round=self.rounds[-1]
                )
                self.rounds.append(challenge_round)

                # Check convergence
                self.state = MatchState.CHECKING_CONVERGENCE
                convergence = await self._check_convergence(
                    challenge_round.model_a_response,
                    challenge_round.model_b_response
                )

                if convergence.score >= self.convergence_threshold:
                    return await self._complete_match(convergence, "converged")

                # Defense round
                self.state = MatchState.DEFENDING
                defense_round = await self._run_defense_round(
                    round_num=cycle * 2 + 2,
                    challenge_round=challenge_round,
                    original_round=self.rounds[0]
                )
                self.rounds.append(defense_round)

                # Check convergence again
                convergence = await self._check_convergence(
                    defense_round.model_a_response,
                    defense_round.model_b_response
                )

                if convergence.score >= self.convergence_threshold:
                    return await self._complete_match(convergence, "converged")

            # Max rounds reached - force decision
            return await self._force_arbiter_decision()

        except Exception as e:
            self.state = MatchState.FAILED
            return await self._handle_failure(e)
```

#### Pipeline Integration:

```python
# src/archguru/agents/pipeline.py
from langgraph.graph import StateGraph, END

def create_debate_pipeline() -> StateGraph:
    """Create the LangGraph pipeline for debate flow"""

    workflow = StateGraph(DebateState)

    # Add nodes
    workflow.add_node("select_models", select_models_node)
    workflow.add_node("run_debate", run_debate_node)
    workflow.add_node("check_tournament", check_tournament_node)
    workflow.add_node("format_output", format_output_node)

    # Define edges
    workflow.add_edge("select_models", "run_debate")
    workflow.add_conditional_edges(
        "run_debate",
        lambda x: "check_tournament" if x.has_more_models else "format_output",
        {
            "check_tournament": "check_tournament",
            "format_output": "format_output"
        }
    )
    workflow.add_edge("check_tournament", "run_debate")
    workflow.add_edge("format_output", END)

    workflow.set_entry_point("select_models")

    return workflow.compile()

async def run_debate_node(state: DebateState) -> DebateState:
    """Node that executes a single debate match"""

    match = DebateMatch(
        model_a=state.current_models[0],
        model_b=state.current_models[1],
        context=state.context,
        previous_matches=state.completed_matches
    )

    result = await match.run()

    state.completed_matches.append(match)
    state.current_winner = result.winner
    state.current_winner_position = result.winner_position

    return state
```

---

### Phase 2: Tournament System (v0.9)

**Timeline:** 2-3 days  
**Goal:** Enable multi-model tournaments with proper bracket management

#### Additional Components:

```
src/archguru/
  ├── tournament/
  │   ├── __init__.py
  │   ├── bracket.py         # Tournament orchestration
  │   ├── scheduler.py       # Budget allocation
  │   └── context.py         # History management
```

#### Tournament Schema:

```sql
CREATE TABLE tournament (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  run_id UUID NOT NULL REFERENCES run(id),

  -- Configuration
  total_models INTEGER NOT NULL,
  bracket_type TEXT DEFAULT 'single_elimination',
  total_budget_usd REAL,

  -- Results
  final_winner_id INTEGER REFERENCES model(id),
  total_matches INTEGER,

  -- Metadata
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  completed_at TIMESTAMP
);

CREATE TABLE tournament_match (
  tournament_id UUID NOT NULL REFERENCES tournament(id),
  match_id UUID NOT NULL REFERENCES debate_match(id),
  match_order INTEGER NOT NULL,
  budget_allocated REAL,

  PRIMARY KEY (tournament_id, match_id)
);
```

#### Tournament Orchestration:

```python
# src/archguru/tournament/bracket.py
class TournamentBracket:
    """Manages single-elimination tournament flow"""

    def __init__(
        self,
        models: List[str],
        context: DecisionContext,
        total_budget: float = 0.50
    ):
        self.models = models
        self.context = context
        self.total_budget = total_budget
        self.matches: List[DebateMatch] = []

        # Front-loaded budget allocation
        self.budget_allocations = self._calculate_budget_allocation(len(models))

    def _calculate_budget_allocation(self, n_models: int) -> List[float]:
        """Front-load budget: 40%, 30%, 20%, 10%"""
        allocations = [0.4, 0.3, 0.2, 0.1]
        return allocations[:n_models-1]

    async def run(self) -> TournamentResult:
        """Execute the tournament"""

        # Initial match
        current_winner, winner_position = await self._run_match(
            self.models[0],
            self.models[1],
            budget=self.total_budget * self.budget_allocations[0]
        )

        # Challenger rounds
        for i, challenger in enumerate(self.models[2:], start=1):
            # Prepare context for challenger
            challenger_context = self._prepare_challenger_context(
                winner=current_winner,
                winner_position=winner_position,
                history=self.matches
            )

            # Run match with history awareness
            try:
                current_winner, winner_position = await self._run_match(
                    current_winner,
                    challenger,
                    budget=self.total_budget * self.budget_allocations[i],
                    context=challenger_context
                )
            except ModelFailure:
                # If winner fails, challenger advances
                current_winner = challenger
                winner_position = await self._get_default_position(challenger)

        return TournamentResult(
            winner=current_winner,
            winner_position=winner_position,
            matches=self.matches,
            total_cost=sum(m.total_cost for m in self.matches)
        )
```

---

### Phase 3: OpenRouter Intelligence (v1.0)

**Timeline:** 3-4 days  
**Goal:** Dynamic model selection using real-time OpenRouter data

#### Intelligence Components:

```
src/archguru/
  ├── intelligence/
  │   ├── __init__.py
  │   ├── openrouter.py      # OR API integration
  │   ├── selector.py        # Model selection logic
  │   ├── cache.py           # OR data caching
  │   └── rankings.py        # Elo rating system
```

#### OpenRouter Cache Schema:

```sql
CREATE TABLE openrouter_model (
  id TEXT PRIMARY KEY,                      -- "anthropic/claude-3-opus"
  name TEXT NOT NULL,
  provider TEXT NOT NULL,

  -- Capabilities
  context_length INTEGER,
  supports_tools BOOLEAN DEFAULT FALSE,
  supports_vision BOOLEAN DEFAULT FALSE,

  -- Pricing (USD per million tokens)
  prompt_price_usd REAL,
  completion_price_usd REAL,

  -- Performance
  or_ranking REAL,                          -- OpenRouter's score
  avg_latency_ms INTEGER,
  availability_score REAL,                  -- 0-1 availability

  -- Metadata
  last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  architecture TEXT,                         -- 'instruct' | 'chat' | 'base'
  training_cutoff DATE
);

CREATE TABLE model_elo_rating (
  model_id TEXT NOT NULL,
  decision_type TEXT NOT NULL,

  -- Elo system
  rating INTEGER DEFAULT 1200,
  matches_played INTEGER DEFAULT 0,
  matches_won INTEGER DEFAULT 0,

  -- Performance metrics
  avg_convergence_speed REAL,               -- Rounds to convergence
  avg_confidence REAL,                      -- Average confidence scores
  challenge_quality_score REAL,             -- How good at challenging

  last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

  PRIMARY KEY (model_id, decision_type)
);
```

#### Model Selection Algorithm:

```python
# src/archguru/intelligence/selector.py
class ModelSelector:
    """Intelligent model selection using OR data + our rankings"""

    async def select_debate_pair(
        self,
        decision_type: str,
        budget: float = 0.30,
        user_models: Optional[List[str]] = None,
        force_diversity: bool = True
    ) -> Tuple[str, str]:
        """Select optimal model pair for debate"""

        # User override
        if user_models and len(user_models) >= 2:
            return await self._validate_models(user_models[:2])

        # Refresh cache if stale
        await self._refresh_cache_if_needed()

        # Get candidates
        candidates = await self._get_suitable_models(decision_type)

        # Score each model
        for model in candidates:
            model.debate_score = self._calculate_debate_score(
                model,
                decision_type
            )

        # Select primary model (strongest within budget)
        model_a = self._select_primary(candidates, budget * 0.6)

        # Select challenger (complementary)
        model_b = self._select_challenger(
            candidates,
            primary=model_a,
            remaining_budget=budget * 0.4,
            force_diversity=force_diversity
        )

        return (model_a.id, model_b.id)

    def _calculate_debate_score(self, model: Model, decision_type: str) -> float:
        """Composite score for debate suitability"""

        # Get our Elo rating for this decision type
        elo_rating = self._get_elo_rating(model.id, decision_type)

        return (
            0.30 * (model.or_ranking / 100) +           # OpenRouter quality
            0.25 * (elo_rating / 1600) +                # Our domain expertise
            0.20 * (1 - self._normalize_price(model)) + # Cost efficiency
            0.15 * model.availability_score +           # Reliability
            0.10 * (1 if model.supports_tools else 0)   # Tool support
        )
```

---

### Phase 4: Production Polish (v1.1)

**Timeline:** Ongoing  
**Goal:** Optimize performance, resilience, and user experience

#### Key Enhancements:

**4.1 - Intelligent Caching**

```python
class DebateCache:
    """Cache debate chains for instant replay"""

    def get_cache_key(self, context: DecisionContext, models: List[str]) -> str:
        # Deterministic key generation
        components = [
            context.decision_type,
            self._normalize_context(context.description),
            sorted(models),
            PROMPT_VERSION
        ]
        return hashlib.sha256(str(components).encode()).hexdigest()

    async def get_or_run_debate(self, match: DebateMatch) -> MatchResult:
        key = self.get_cache_key(match.context, [match.model_a, match.model_b])

        # Check cache
        cached = await self.get(key)
        if cached and not match.context.force_fresh:
            return cached

        # Run debate
        result = await match.run()

        # Cache for 7 days
        await self.set(key, result, ttl=7*24*3600)

        return result
```

**4.2 - Failure Recovery**

```python
class FailureHandler:
    """Graceful failure handling throughout debates"""

    async def handle_model_failure(
        self,
        match: DebateMatch,
        failed_model: str,
        error: Exception
    ) -> MatchResult:

        # Round 0 failure - need backup
        if match.state == MatchState.RESEARCHING:
            backup = await self.get_backup_model(failed_model)
            return await match.restart_with_model(backup)

        # Mid-debate failure - opponent wins
        elif match.state in [MatchState.CHALLENGING, MatchState.DEFENDING]:
            winner = match.model_a if failed_model == match.model_b else match.model_b
            return MatchResult(
                winner=winner,
                reason=f"Opponent {failed_model} failed: {error}",
                debate_status="opponent_failed"
            )

        # Tournament failure - advance to next challenger
        elif match.previous_matches:
            return MatchResult(
                winner=None,
                advance_to_next=True,
                reason="Skip to next challenger"
            )
```

**4.3 - Export Formats**

```python
class DebateExporter:
    """Export debates in various formats"""

    def to_markdown(self, match: DebateMatch) -> str:
        """Full debate transcript in Markdown"""

    def to_summary(self, match: DebateMatch) -> str:
        """Executive summary with key points"""

    def to_mermaid(self, tournament: TournamentResult) -> str:
        """Tournament bracket visualization"""

    def to_decision_record(self, result: MatchResult) -> str:
        """Formal decision record format"""
```

## Testing Strategy

```python
# tests/test_debate.py
async def test_convergence_within_three_rounds():
    """Verify models converge efficiently"""
    match = DebateMatch("gpt-4", "claude-3", test_context)
    result = await match.run()

    assert len(match.rounds) <= 6  # 3 cycles max
    assert result.final_convergence_score >= 80

async def test_tournament_budget_allocation():
    """Verify budget properly distributed"""
    bracket = TournamentBracket(
        ["gpt-4", "claude-3", "llama-3"],
        test_context,
        total_budget=0.50
    )
    result = await bracket.run()

    assert result.matches[0].allocated_budget == 0.20  # 40% of 0.50
    assert result.matches[1].allocated_budget == 0.15  # 30% of 0.50
    assert result.total_cost <= 0.50

async def test_non_convergent_debate_handling():
    """Verify graceful handling of persistent disagreement"""
    # Use models known to disagree
    match = DebateMatch("gpt-4", "llama-3", controversial_context)
    result = await match.run()

    assert result.debate_status == "forced"
    assert result.arbiter_reasoning is not None
```

## Success Metrics

- **Convergence Rate**: >60% within 3 rounds
- **Debate Latency**: <3 minutes for 2 models
- **Tournament Latency**: <5 minutes for 3 models
- **Cache Hit Rate**: >40% after first week
- **Model Selection Accuracy**: >80% user satisfaction
- **Cost Prediction Accuracy**: ±10% of actual
