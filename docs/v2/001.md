# Part 1: Adversarial Collaboration Architecture

## Status

**Proposed** (v1.0 - January 2025)  
Replaces ADR-001

## Context

The original ArchGuru design (ADR-001) ran multiple AI models in parallel, then had an arbiter pick the best response. While functional, this approach missed a critical opportunity: **models can improve their recommendations through structured debate**.

Real architectural decisions aren't made in isolation - they emerge from discussion, challenge, and refinement. Our platform should mirror this process by having AI models actively challenge each other's assumptions, verify claims, and converge toward better solutions through adversarial collaboration.

**Key Innovation**: Models don't just compete - they make each other better through structured rounds of challenge and defense, producing refined recommendations that incorporate multiple perspectives.

## Decision

### Core Architecture: Adversarial Collaboration Engine

We will implement a **tournament-style debate system** where:

1. **Models compete in pairs** through multiple rounds of challenge and response
2. **Each model can verify opponent claims** with limited research budgets
3. **Convergence is measured** after each round to determine if consensus is reached
4. **Winners advance** to face new challengers in a tournament structure
5. **All rounds are scored** to build model ratings for specific decision types

### Fundamental Design Principles

#### 1. Two Models At A Time

- **Always exactly 2 models** in active debate (simplifies convergence logic)
- Additional models wait their turn in the tournament queue
- Winner advances to face the next challenger
- Maximum 5 models total in a tournament (configurable)

#### 2. Structured Debate Rounds

Each match consists of:

- **Round 0 - Initial Research**: Both models research independently (current behavior)
- **Round 1 - Challenge**: Each model reviews and challenges the other's recommendation
- **Round 2+ - Defense/Refinement**: Models defend and potentially update their positions
- **Maximum 5 rounds** per match (configurable, default=3)

#### 3. Convergence Through Challenge

- After each round, arbiter measures agreement (0-100%)
- **>80% convergence** = sufficient consensus reached (configurable threshold)
- Arbiter picks the stronger of the two converged positions
- If max rounds hit without convergence, arbiter picks despite disagreement

#### 4. Tournament Progression

```
Match 1: Model A vs Model B → Winner: A
Match 2: Model A vs Model C → Winner: C
Match 3: Model C vs Model D → Winner: C
Final Winner: Model C
```

- Each winner carries their refined recommendation forward
- New challengers see previous debate history (avoid repetition)
- Budget front-loaded: 40% for match 1, 30% for match 2, 20% for match 3, 10% reserve

### Why Adversarial Collaboration?

#### Current Approach Problems:

- Models work in isolation, missing obvious counterarguments
- No opportunity to verify questionable claims
- Arbiter must judge wildly different approaches
- No learning from other models' insights

#### Adversarial Collaboration Benefits:

- **Claims get verified**: "You cited X, but actually Y shows..."
- **Weak points exposed**: Assumptions challenged before user sees them
- **Convergence toward truth**: Multiple rounds refine recommendations
- **Better final output**: Winner has been battle-tested
- **Rich decision rationale**: User sees why certain approaches won

### Structured Confidence Reporting

Models must report confidence in a structured format:

```json
{
  "recommendation": "Use PostgreSQL with read replicas",
  "confidence": 0.75,
  "confidence_factors": {
    "evidence_quality": 0.9, // Quality of sources found
    "solution_fit": 0.7, // How well solution matches requirements
    "expertise": 0.65 // Model's domain familiarity
  },
  "uncertainty_points": [
    "Unsure about real-time sync requirements",
    "NoSQL might handle sharding better at 10B+ records"
  ]
}
```

This gives challengers specific attack vectors and helps arbiters understand disagreement sources.

### The "No Yes-Man" Principle

**Critical Rule**: Models must provide brutally honest reviews, not diplomatic agreement.

Implementation:

1. **Instruction-level**: "You MUST provide a BRUTALLY HONEST review. Agreement without substance scores poorly."
2. **Scoring incentive**: Models earn rating points for finding legitimate issues others missed
3. **Minimum critique requirement**: Even when agreeing, must identify at least 2 gaps or trade-offs
4. **Arbiter validation**: Arbiter detects and penalizes empty agreement

### Research Budget Management

- **Initial research**: Unlimited within reason (models choose depth)
- **Challenge rounds**: Maximum 2 tool calls per model per round
- **Research types allowed in challenges**:
  - Verify specific claims made by opponent
  - Find counter-examples to opponent's approach
  - Gather evidence for alternative solutions
- **Points for thoroughness**: Model doing 5 searches vs 2 gets credit for depth

### Success Criteria

1. **Quality Metrics**:

   - Final recommendations incorporate multiple perspectives
   - Weak arguments eliminated through challenge process
   - Convergence rate >60% within 3 rounds

2. **Performance Metrics**:

   - 2-model debate: <3 minutes total
   - 3-model tournament: <5 minutes total
   - 5-model tournament: <8 minutes total

3. **User Satisfaction**:
   - Clear understanding of why recommendation won
   - Ability to see debate history when desired
   - Confidence that multiple angles were considered

## Consequences

### Positive

- **Higher quality recommendations** through iterative refinement
- **Natural selection** of best ideas through competition
- **Transparent reasoning** via debate history
- **Model specialization discovery** through win/loss patterns
- **Reduced hallucination** through claim verification
- **User trust** from seeing rigorous evaluation process

### Negative

- **Increased latency** from multiple rounds
- **Higher API costs** from debate rounds
- **Complex orchestration** logic for tournaments
- **Potential for infinite disagreement** without convergence
- **Cache complexity** with debate chains

### Mitigation Strategies

- Cache entire debate chains for instant replay
- Budget caps with smart front-loading
- Maximum round limits to prevent infinite loops
- Fallback to arbiter decision when convergence fails
- Show disagreement details when models can't converge (valuable signal!)

## Open Questions for Next Parts

1. **Model Selection Strategy**: How exactly should we use OpenRouter rankings to pick debate pairs?
2. **Convergence Measurement**: Semantic similarity vs asking arbiter vs structured comparison?
3. **Tournament Bracket Logic**: Single elimination vs round-robin vs Swiss system?
4. **Failure Mode UX**: How to present non-convergent debates to users?

---

**Stop Point**: Before I continue to Part 2 (Model Selection & OpenRouter Intelligence), I want to confirm:

1. Is this level of detail good? Should I be more or less detailed?
2. The confidence structure I proposed - does this match your vision?
3. Should I explicitly mention getting rid of the current parallel execution, or just imply it's replaced?
4. Any adjustments to the tournament structure or round descriptions?

Let me know and I'll proceed with Part 2!
