# Part 5: Technical Schemas & Interfaces

## Status

**Proposed** (v1.0 - January 2025)  
Technical specifications for ADR-002, ADR-003, ADR-004

## Context

This document provides the detailed technical interfaces, data structures, and API contracts needed to implement the adversarial collaboration system.

## Core Data Structures

### Response Format

```python
# src/archguru/models/response.py
from dataclasses import dataclass
from typing import Dict, List, Optional, Any

@dataclass
class ConfidenceFactors:
    """Structured confidence breakdown"""
    evidence_quality: float      # 0-1: Quality of sources/research
    solution_fit: float          # 0-1: How well solution matches requirements
    expertise: float             # 0-1: Model's domain familiarity

    def overall(self) -> float:
        """Weighted average of factors"""
        return (
            self.evidence_quality * 0.4 +
            self.solution_fit * 0.4 +
            self.expertise * 0.2
        )

@dataclass
class ModelResponse:
    """Structured response from a model"""
    recommendation: str                          # The actual recommendation
    confidence: float                           # Overall confidence (0-1)
    confidence_factors: ConfidenceFactors      # Detailed breakdown
    uncertainty_points: List[str]              # Specific areas of uncertainty

    # Supporting data
    key_facts: List[str]                       # Main supporting facts
    sources: List[ToolCallResult]              # Research performed
    trade_offs: Dict[str, str]                 # Identified trade-offs

    # Metadata
    model_id: str
    tokens_used: int
    response_time_ms: int

    def to_challenge_context(self) -> Dict[str, Any]:
        """Format for challenger to review"""
        return {
            'recommendation': self.recommendation,
            'confidence': self.confidence,
            'weak_points': self.uncertainty_points,
            'claimed_facts': self.key_facts,
            'sources_to_verify': [s.claim for s in self.sources]
        }
```

### Convergence Measurement

```python
# src/archguru/debate/convergence.py
from enum import Enum

class ConvergenceMethod(Enum):
    STRUCTURAL = "structural"    # Text similarity
    SEMANTIC = "semantic"        # Embedding similarity
    ARBITER = "arbiter"         # LLM judgment

@dataclass
class ConvergenceResult:
    """Result of convergence measurement"""
    score: float                                # 0-100 percentage
    method: ConvergenceMethod

    # Detailed breakdown (when method=ARBITER)
    core_recommendation_alignment: Optional[float] = None
    reasoning_alignment: Optional[float] = None
    tradeoff_alignment: Optional[float] = None
    implementation_alignment: Optional[float] = None

    # Key differences identified
    key_disagreements: List[str] = field(default_factory=list)

    def is_converged(self, threshold: float = 80.0) -> bool:
        return self.score >= threshold

    def get_disagreement_summary(self) -> str:
        """Human-readable disagreement summary"""
        if self.score > 90:
            return "Models strongly agree"
        elif self.score > 70:
            return f"Models mostly agree, differ on: {', '.join(self.key_disagreements[:2])}"
        else:
            return f"Fundamental disagreement on: {', '.join(self.key_disagreements[:3])}"
```

### Match Results

```python
# src/archguru/debate/results.py
@dataclass
class MatchResult:
    """Result of a single debate match"""
    winner: str                                 # Model ID
    winner_position: ModelResponse
    loser: str
    loser_position: ModelResponse

    # Debate metrics
    rounds_completed: int
    convergence_scores: List[float]            # Score after each round
    final_convergence: float
    debate_status: str                         # 'converged' | 'forced' | 'failed'

    # Arbiter analysis
    arbiter_reasoning: Optional[str] = None
    winner_strengths: List[str] = field(default_factory=list)
    loser_weaknesses: List[str] = field(default_factory=list)

    # Cost tracking
    total_cost_usd: float
    model_a_cost: float
    model_b_cost: float

    # For non-convergent debates
    is_disputed: bool = False
    dispute_reason: Optional[str] = None

@dataclass
class TournamentResult:
    """Result of a complete tournament"""
    final_winner: str
    final_recommendation: ModelResponse

    # Match history
    matches: List[MatchResult]
    elimination_order: List[str]               # Models in order of elimination

    # Aggregate metrics
    total_rounds: int
    total_cost_usd: float
    average_convergence: float

    # Insights
    strongest_challenge: Optional[str] = None  # Most effective challenge made
    key_refinements: List[str] = field(default_factory=list)  # How position improved
```

## API Interfaces

### CLI Interface

```python
# src/archguru/cli/commands.py
from typing import Optional, List
import click

@click.command()
@click.option('--decision-type', '-t', required=True,
              type=click.Choice(['database', 'cache', 'messaging', 'api', 'storage']))
@click.option('--context', '-c', required=True, help='Decision context')
@click.option('--models', '-m', multiple=True, help='Specific models to use')
@click.option('--tournament-size', '-n', default=2, help='Number of models in tournament')
@click.option('--budget', '-b', type=float, default=0.30, help='Maximum budget in USD')
@click.option('--max-rounds', default=3, help='Maximum debate rounds per match')
@click.option('--convergence-threshold', default=80, help='Convergence percentage required')
@click.option('--force-diversity', is_flag=True, help='Force different providers')
@click.option('--show-debate', is_flag=True, help='Show full debate transcript')
@click.option('--show-selection', is_flag=True, help='Show model selection reasoning')
@click.option('--export-format', type=click.Choice(['markdown', 'json', 'summary']))
@click.option('--no-cache', is_flag=True, help='Skip cache lookup')
async def debate(
    decision_type: str,
    context: str,
    models: Optional[List[str]],
    tournament_size: int,
    budget: float,
    max_rounds: int,
    convergence_threshold: int,
    force_diversity: bool,
    show_debate: bool,
    show_selection: bool,
    export_format: Optional[str],
    no_cache: bool
):
    """Run adversarial collaboration debate for architectural decision"""

    # Implementation shown in previous parts
    pass
```

### LangGraph State Definition

```python
# src/archguru/agents/state.py
from typing import TypedDict, List, Optional, Dict, Any

class DebateState(TypedDict):
    """State for debate pipeline"""

    # Input
    decision_type: str
    context: str
    user_models: Optional[List[str]]
    budget: float
    max_rounds: int
    convergence_threshold: float
    force_diversity: bool

    # Model selection
    selected_models: List[str]
    selection_reasoning: str
    estimated_cost: float

    # Tournament state
    tournament_bracket: List[Tuple[str, str]]  # Planned matches
    current_match_index: int
    completed_matches: List[MatchResult]

    # Current match state
    current_models: Tuple[str, str]
    current_winner: Optional[str]
    current_winner_position: Optional[ModelResponse]
    debate_history: List[Dict[str, Any]]       # For challenger context

    # Results
    final_winner: Optional[str]
    final_recommendation: Optional[ModelResponse]
    total_cost: float

    # Metadata
    run_id: str
    start_time: float
    end_time: Optional[float]
```

### Tool Call Interface

```python
# src/archguru/tools/interface.py
from abc import ABC, abstractmethod

class ResearchTool(ABC):
    """Base interface for research tools"""

    @abstractmethod
    async def search(self, query: str, context: Dict[str, Any]) -> ToolCallResult:
        """Execute a research query"""
        pass

    @abstractmethod
    def estimate_cost(self) -> float:
        """Estimate cost of tool call"""
        pass

@dataclass
class ToolCallResult:
    """Result from a research tool call"""
    tool_name: str
    query: str
    results: List[Dict[str, Any]]

    # For verification
    claim: str                                  # What this proves/supports
    confidence: float                           # How confident in the results

    # Metadata
    execution_time_ms: int
    tokens_used: int
    cost_usd: float

    def to_challenge_evidence(self) -> str:
        """Format for use in challenges"""
        return f"[{self.tool_name}] {self.claim} (confidence: {self.confidence:.0%})"
```

## Prompt Templates

### Challenge Prompt Template

```python
# src/archguru/debate/prompts.py
CHALLENGE_PROMPT = """You are participating in an adversarial collaboration debate about {decision_type}.

## Your Opponent's Position

**Recommendation:** {opponent_recommendation}

**Confidence:** {opponent_confidence:.0%}
- Evidence Quality: {evidence_quality:.0%}
- Solution Fit: {solution_fit:.0%}
- Domain Expertise: {expertise:.0%}

**Key Claims:**
{opponent_claims}

**Sources Used:**
{opponent_sources}

**Admitted Uncertainties:**
{opponent_uncertainties}

## Your Task

Provide a BRUTALLY HONEST review of their recommendation. You must:

1. **Identify at least 2 significant weaknesses or gaps** in their analysis
2. **Challenge questionable claims** with counter-evidence
3. **Propose superior alternatives** if they exist
4. **Acknowledge strong points** (but don't be a yes-man)

Focus especially on their weak areas: {opponent_uncertainties}

You may run up to 2 research queries to:
- Verify specific claims they made
- Find counter-examples
- Support alternative approaches

Remember:
- Agreement without substance scores poorly
- Finding legitimate issues others missed improves your rating
- Be specific and evidence-based in your challenges

## Response Format

Structure your response as:

### Strengths (be brief)
- What they got right

### Critical Weaknesses
1. [Specific weakness with evidence]
2. [Another weakness with evidence]

### Questionable Claims
- [Claim]: [Why it's questionable + counter-evidence]

### Superior Alternative (if applicable)
- [Your alternative]: [Why it's better]

### Research Performed
- [What you verified and found]
"""

DEFENSE_PROMPT = """Your architectural recommendation has been challenged in a debate.

## Your Original Position
{original_recommendation}

## Challenges Raised
{challenges}

## Your Task

Respond to these challenges by:

1. **Defending valid points** with additional evidence
2. **Acknowledging legitimate weaknesses** honestly
3. **Updating your recommendation** if the challenges are valid
4. **Maintaining your position** with justification if the challenges are weak

You may run up to 2 research queries to support your defense.

## Response Format

### Response to Challenges
[Address each challenge specifically]

### Position Update
**Maintaining/Updating:** [State clearly]
**Recommendation:** [Your potentially updated recommendation]
**What Changed:** [If updated, explain what and why]

### Confidence Update
Overall: {new_confidence}%
- Evidence Quality: {new_evidence}%
- Solution Fit: {new_fit}%
- Expertise: {new_expertise}%

### New Evidence (if any)
[Research results that support your position]
"""
```

## Configuration Schema

### YAML Configuration

```yaml
# config/debate.yaml
debate:
  defaults:
    max_rounds: 3
    convergence_threshold: 80
    budget_usd: 0.30
    force_diversity: true

  tournament:
    max_models: 5
    budget_allocation: [0.4, 0.3, 0.2, 0.1] # Front-loaded

  convergence:
    methods:
      - structural # Fast check first
      - semantic # Medium cost
      - arbiter # Most accurate
    structural_threshold: 95 # Skip other methods if this high
    semantic_threshold: 90

  challenges:
    max_research_queries: 2
    min_weaknesses_required: 2
    no_yes_man_enforcement: true

  models:
    refresh_cache_hours: 24
    min_context_length: 16000
    require_tool_support: true

  scoring:
    # For Elo updates
    initial_rating: 1200
    k_factor: 32

    # Composite debate score weights
    weights:
      initial_response: 0.4
      challenge_quality: 0.3
      defense_adaptation: 0.3

  output:
    default_format: summary
    show_convergence_progression: true
    show_cost_breakdown: false

  caching:
    enabled: true
    ttl_days: 7
    compression: true
```

### Environment Variables

```bash
# .env configuration
ARCHGURU_DEBATE_MODE=true
ARCHGURU_MAX_ROUNDS=3
ARCHGURU_CONVERGENCE_THRESHOLD=80
ARCHGURU_DEFAULT_BUDGET=0.30
ARCHGURU_CACHE_DIR=/tmp/archguru_cache
ARCHGURU_OPENROUTER_API_KEY=sk-or-...
ARCHGURU_DATABASE_URL=postgresql://...
ARCHGURU_REDIS_URL=redis://...
```

## Error Handling

```python
# src/archguru/exceptions.py
class DebateException(Exception):
    """Base exception for debate system"""
    pass

class ConvergenceFailure(DebateException):
    """Models failed to converge after max rounds"""
    def __init__(self, match: DebateMatch):
        self.match = match
        self.final_scores = [r.convergence_score for r in match.rounds]
        super().__init__(
            f"Failed to converge after {len(match.rounds)} rounds. "
            f"Final score: {self.final_scores[-1]:.0f}%"
        )

class ModelSelectionFailure(DebateException):
    """Could not select suitable models"""
    def __init__(self, reason: str, available_budget: float):
        self.reason = reason
        self.budget = available_budget
        super().__init__(f"Model selection failed: {reason} (budget: ${available_budget:.2f})")

class BudgetExceeded(DebateException):
    """Estimated cost exceeds budget"""
    def __init__(self, estimated: float, limit: float):
        self.estimated = estimated
        self.limit = limit
        self.overage = estimated - limit
        super().__init__(
            f"Estimated cost ${estimated:.2f} exceeds budget ${limit:.2f} "
            f"by ${self.overage:.2f}"
        )

class TournamentFailure(DebateException):
    """Tournament could not complete"""
    def __init__(self, completed_matches: int, total_planned: int, reason: str):
        self.completed = completed_matches
        self.planned = total_planned
        self.reason = reason
        super().__init__(
            f"Tournament failed after {completed}/{planned} matches: {reason}"
        )
```

## Monitoring & Metrics

```python
# src/archguru/metrics/tracker.py
class DebateMetrics:
    """Track debate system performance"""

    async def record_match(self, match: MatchResult):
        """Record match metrics"""
        await self.db.execute("""
            INSERT INTO debate_metrics (
                match_id,
                convergence_speed,
                final_convergence,
                rounds_used,
                cost_usd,
                duration_ms,
                model_a_id,
                model_b_id,
                winner_id,
                decision_type
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, ...)

        # Update Elo ratings
        await self.update_elo_ratings(match)

    async def get_model_stats(self, model_id: str, decision_type: str):
        """Get model performance statistics"""
        return await self.db.fetchone("""
            SELECT
                rating,
                matches_played,
                matches_won,
                avg_convergence_speed,
                avg_confidence,
                challenge_quality_score
            FROM model_elo_rating
            WHERE model_id = ? AND decision_type = ?
        """, model_id, decision_type)

    async def get_system_health(self):
        """System health metrics"""
        return {
            'avg_convergence_rate': await self.get_convergence_rate(),
            'avg_rounds_to_converge': await self.get_avg_rounds(),
            'avg_debate_cost': await self.get_avg_cost(),
            'cache_hit_rate': await self.get_cache_hit_rate(),
            'model_failure_rate': await self.get_failure_rate()
        }
```

## Testing Utilities

```python
# tests/fixtures.py
import pytest
from unittest.mock import AsyncMock

@pytest.fixture
async def mock_debate_match():
    """Create a mock debate match for testing"""
    match = DebateMatch(
        model_a="gpt-4",
        model_b="claude-3",
        context=DecisionContext(
            decision_type="database",
            description="Choose DB for high-scale app"
        )
    )

    # Mock the LLM calls
    match._call_model = AsyncMock(return_value=ModelResponse(...))

    return match

@pytest.fixture
def convergence_test_cases():
    """Test cases for convergence scenarios"""
    return [
        # High convergence
        {
            'response_a': "Use PostgreSQL with read replicas",
            'response_b': "PostgreSQL with read replicas is best",
            'expected_score': 95,
            'expected_method': 'structural'
        },
        # Medium convergence
        {
            'response_a': "Use PostgreSQL for consistency",
            'response_b': "Consider MongoDB for scale, but PostgreSQL works",
            'expected_score': 60,
            'expected_method': 'arbiter'
        },
        # Low convergence
        {
            'response_a': "PostgreSQL is the clear choice",
            'response_b': "MongoDB is far superior here",
            'expected_score': 20,
            'expected_method': 'arbiter'
        }
    ]
```
