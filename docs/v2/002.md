# Part 2: Model Selection & OpenRouter Intelligence

## Status

**Proposed** (v1.0 - January 2025)  
Companion to ADR-002

## Context

Static model presets are outdated the moment they're written. OpenRouter provides real-time intelligence about model performance, pricing, and availability. We should leverage this dynamic data to make intelligent model selection decisions that optimize for quality, cost, and diversity of perspective.

Additionally, our accumulated match history provides domain-specific intelligence about which models excel at particular architectural decisions. Combining OpenRouter's global data with our specialized knowledge creates a powerful selection system.

## Decision

### Core Model Selection Strategy

Replace static presets with a **dynamic selection algorithm** that combines:

1. **OpenRouter real-time data** (pricing, rankings, capabilities, availability)
2. **Our domain-specific Elo ratings** per decision type
3. **Budget-aware optimization** with user control
4. **Diversity requirements** for productive debates

### OpenRouter Data Integration

#### Data Cache Schema

```sql
CREATE TABLE openrouter_model (
  id TEXT PRIMARY KEY,                    -- "openai/gpt-4o"
  name TEXT,                              -- "GPT-4 Optimized"
  description TEXT,
  context_length INTEGER,
  prompt_price_per_million REAL,         -- USD per million tokens
  completion_price_per_million REAL,
  or_ranking REAL,                       -- OpenRouter's internal score
  architecture TEXT,                     -- "chat" | "instruct" | "base"
  top_provider TEXT,                     -- "openai" | "anthropic" | "meta"
  capabilities JSONB,                    -- {"tools": true, "vision": false, ...}
  avg_latency_ms INTEGER,                -- Average response time
  availability_score REAL,               -- 0-1, rolling 24h uptime
  last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_or_ranking ON openrouter_model(or_ranking DESC);
CREATE INDEX idx_debate_suitability ON openrouter_model(
  (capabilities->>'tools')::boolean DESC,
  context_length DESC,
  or_ranking DESC
);
```

#### Refresh Strategy

- **Auto-refresh** if cache >24 hours old
- **Manual refresh** via `--refresh-models` flag
- **Fallback data** stored for when OpenRouter API is down
- **Granular updates** - only refresh changed fields to minimize API calls

```python
async def refresh_openrouter_cache():
    """Fetch latest model intelligence from OpenRouter"""
    try:
        # Get model list with capabilities
        models = await fetch_openrouter_models()

        # Get current pricing
        pricing = await fetch_openrouter_pricing()

        # Get performance metrics (if available)
        metrics = await fetch_openrouter_metrics()

        # Merge and store
        for model in models:
            model['effective_score'] = calculate_debate_suitability(model)
            await upsert_model_cache(model)

    except OpenRouterAPIError:
        logger.warning("Using cached OpenRouter data")
        # Continue with stale cache rather than failing
```

### Intelligent Pairing Algorithm

#### Selection Criteria Hierarchy

1. **Hard Requirements** (must have):

   - Tool/function calling support (for research)
   - Minimum 16K context (for multi-round debates)
   - Available (>0.8 availability score)

2. **Optimization Factors** (weighted):

   ```python
   debate_suitability_score = (
       0.30 * or_ranking +                    # OpenRouter's quality score
       0.25 * our_elo_rating / 1600 +        # Our domain expertise
       0.20 * (1 - normalized_price) +       # Cost efficiency
       0.15 * instruction_following_score +   # Critical for challenges
       0.10 * (matches_played > 5 ? 1 : 0.5) # Proven track record
   )
   ```

3. **Diversity Requirements**:
   - Different providers preferred (OpenAI vs Anthropic vs Meta)
   - Different model sizes (large vs efficient)
   - Different training cutoffs (for complementary knowledge)

#### The Selection Algorithm

```python
async def select_debate_models(
    decision_type: str,
    user_models: Optional[List[str]] = None,
    budget_usd: float = 0.30,
    force_diversity: bool = True
) -> Tuple[str, str]:
    """Select optimal model pair for adversarial debate"""

    # User override takes precedence
    if user_models and len(user_models) >= 2:
        return validate_and_return(user_models[:2])

    # Refresh cache if stale
    await refresh_cache_if_needed()

    # Get candidate models
    candidates = await get_debate_suitable_models(decision_type)

    # Step 1: Pick the strongest model within budget
    model1 = select_best_within_budget(
        candidates,
        budget_usd * 0.6  # Allow up to 60% for strong model
    )

    # Step 2: Pick complementary challenger
    model2 = select_complementary_model(
        candidates,
        exclude=model1,
        remaining_budget=budget_usd - model1.estimated_cost,
        force_different_provider=force_diversity
    )

    # Fallback if no good pair found
    if not model2:
        model2 = select_cheapest_capable_model(candidates, exclude=model1)

    return (model1.id, model2.id)
```

### Budget Management System

#### Cost Calculation

```python
def estimate_debate_cost(model1: str, model2: str, max_rounds: int = 3) -> float:
    """Estimate total cost for a debate"""
    # Assumptions based on empirical data
    AVG_PROMPT_TOKENS = 2000    # Context + instructions
    AVG_COMPLETION_TOKENS = 800  # Response length
    AVG_TOOL_TOKENS = 500        # Research results

    # Round multipliers (later rounds are shorter due to convergence)
    round_multipliers = [1.0, 0.8, 0.6, 0.5, 0.4]

    total_cost = 0
    for round in range(max_rounds):
        multiplier = round_multipliers[min(round, 4)]

        # Both models contribute to each round
        for model in [model1, model2]:
            prompt_cost = model.prompt_price * AVG_PROMPT_TOKENS * multiplier
            completion_cost = model.completion_price * AVG_COMPLETION_TOKENS * multiplier
            tool_cost = model.prompt_price * AVG_TOOL_TOKENS * 0.5 * multiplier  # Not all rounds use tools

            total_cost += (prompt_cost + completion_cost + tool_cost) / 1_000_000

    return total_cost
```

#### Budget Enforcement

```python
class BudgetExceededError(Exception):
    def __init__(self, estimated: float, limit: float, models: List[str]):
        self.estimated = estimated
        self.limit = limit
        self.models = models
        super().__init__(f"Estimated cost ${estimated:.2f} exceeds limit ${limit:.2f}")

async def enforce_budget(models: List[str], budget: float, decision_type: str):
    """Check and enforce budget before starting debate"""
    estimated = estimate_debate_cost(models[0], models[1])

    if estimated > budget * 1.2:  # 20% tolerance
        # Try to find cheaper alternatives
        alternatives = await find_cheaper_alternatives(decision_type, budget)
        if alternatives:
            print(f"⚠️ Budget exceeded. Suggested alternatives:")
            for alt in alternatives:
                print(f"  - {alt.model1} vs {alt.model2}: ~${alt.cost:.2f}")

            if not confirm("Use original models anyway?"):
                return alternatives[0]

        raise BudgetExceededError(estimated, budget, models)

    elif estimated > budget:
        print(f"⚠️ Estimated cost ${estimated:.2f} slightly exceeds budget ${budget:.2f}")
        return confirm("Continue anyway?")

    return True
```

### Tournament Model Selection

For tournaments with 3+ models:

```python
async def select_tournament_bracket(
    decision_type: str,
    n_models: int = 3,
    total_budget: float = 0.50
) -> List[str]:
    """Select models for tournament bracket"""

    selected = []
    budget_allocations = [0.4, 0.3, 0.2, 0.1]  # Front-loaded

    # First pair: Strongest available models
    pair1_budget = total_budget * budget_allocations[0]
    model1, model2 = await select_debate_models(decision_type, budget_usd=pair1_budget)
    selected.extend([model1, model2])

    # Additional challengers: Diverse perspectives
    remaining_slots = n_models - 2
    for i in range(remaining_slots):
        slot_budget = total_budget * budget_allocations[i + 1]

        # Pick models that are:
        # 1. Different from already selected
        # 2. Have different strengths
        # 3. Within budget
        challenger = await select_challenger(
            decision_type,
            exclude=selected,
            budget=slot_budget,
            optimize_for='diversity'  # vs 'strength'
        )
        selected.append(challenger)

    return selected
```

### Model Ranking Display

```python
async def get_model_recommendations(decision_type: str) -> str:
    """Show recommended models for a decision type"""

    models = await db.query("""
        SELECT
            om.*,
            COALESCE(mr.rating, 1200) as elo,
            COALESCE(mr.matches, 0) as matches,
            om.prompt_price_per_million + om.completion_price_per_million as total_price
        FROM openrouter_model om
        LEFT JOIN model_rating mr ON om.id = mr.model_id
            AND mr.decision_type_id = ? AND mr.algo = 'elo'
        WHERE om.capabilities->>'tools' = 'true'
        ORDER BY
            -- Composite score
            (om.or_ranking * 0.4 + COALESCE(mr.rating/2000, 0.6) * 0.6) DESC
        LIMIT 10
    """, decision_type)

    output = f"🏆 Top models for {decision_type} decisions:\n\n"
    for i, m in enumerate(models, 1):
        output += f"{i}. {m.id}\n"
        output += f"   OR Score: {m.or_ranking:.1f} | "
        output += f"Elo: {m.elo:.0f} ({m.matches} matches) | "
        output += f"Cost: ${m.total_price/1000:.4f}/1K tokens\n"

        # Show specialization
        if m.matches > 10:
            if m.elo > 1400:
                output += f"   ✨ Proven expert in {decision_type}\n"
            elif m.elo < 1000:
                output += f"   ⚠️ Struggles with {decision_type}\n"

    return output
```

## Consequences

### Positive

- **Always optimal model selection** based on real-time data
- **Cost transparency** with accurate estimates
- **Automatic adaptation** to new models and price changes
- **Domain expertise** factored into selection
- **Provider diversity** for richer debates

### Negative

- **Dependency on OpenRouter API** for model data
- **Cache complexity** with refresh logic
- **Selection computation overhead** (minimal, <100ms)

### Mitigation

- Fallback to cached data when OR is down
- Pre-compute selection rankings hourly
- User override always available
