# Part 3: Tournament Progression & Convergence

## Status

**Proposed** (v1.0 - January 2025)  
Companion to ADR-002, ADR-003

## Context

With models selected intelligently (ADR-003) and the adversarial collaboration framework defined (ADR-002), we need precise mechanics for how debates progress, how convergence is measured, and how tournaments handle multiple models. This ADR defines the operational details of the debate engine.

## Decision

### Tournament Structure

#### Single Elimination Bracket

```
                    [Final Winner]
                          |
                    Match 3 (Final)
                    Model C vs D
                      /        \
            Match 1              Match 2
          Model A vs B        Winner vs C
              |                    |
         Model A wins         Model C wins
```

**Key Properties**:

- **N-1 matches** for N models
- **Winner advances** with refined position
- **History carries forward** to avoid repetition
- **Budget front-loaded**: 40% match 1, 30% match 2, 20% match 3, 10% reserve

#### Match Mechanics

Each match follows this state machine:

```python
class MatchState(Enum):
    INITIAL_RESEARCH = "initial_research"      # Round 0
    INITIAL_POSITIONS = "initial_positions"    # Models state positions
    CHALLENGE_ROUND = "challenge_round"        # Round 1, 3, 5...
    DEFENSE_ROUND = "defense_round"            # Round 2, 4, 6...
    CONVERGENCE_CHECK = "convergence_check"    # After each round
    MATCH_COMPLETE = "match_complete"          # Winner selected

class DebateMatch:
    def __init__(self, model_a: str, model_b: str, history: List[Match] = None):
        self.model_a = model_a
        self.model_b = model_b
        self.history = history or []  # Previous matches for context
        self.rounds: List[DebateRound] = []
        self.max_rounds = 3  # Configurable, default 3
        self.convergence_threshold = 80  # Configurable, default 80%
```

### Convergence Measurement System

#### Three-Layer Convergence Check

```python
async def measure_convergence(
    response_a: str,
    response_b: str,
    arbiter_model: str
) -> ConvergenceResult:
    """Multi-method convergence measurement"""

    # Layer 1: Structural alignment (fast, cheap)
    structural_score = check_structural_alignment(response_a, response_b)
    if structural_score > 95:  # Nearly identical
        return ConvergenceResult(score=structural_score, method="structural")

    # Layer 2: Semantic similarity (medium cost)
    semantic_score = await check_semantic_similarity(response_a, response_b)
    if semantic_score > 90:  # Very similar meaning
        return ConvergenceResult(score=semantic_score, method="semantic")

    # Layer 3: Arbiter judgment (highest cost, most accurate)
    arbiter_prompt = f"""
    Analyze the agreement level between these two architectural recommendations.

    Response A: {response_a}
    Response B: {response_b}

    Rate their convergence on:
    1. Core recommendation (same solution?): 0-100
    2. Key reasoning points (same logic?): 0-100
    3. Trade-off identification (same concerns?): 0-100
    4. Implementation approach (same steps?): 0-100

    Return ONLY a JSON object:
    {{
        "overall_convergence": <0-100>,
        "core_recommendation": <0-100>,
        "reasoning_alignment": <0-100>,
        "tradeoff_alignment": <0-100>,
        "implementation_alignment": <0-100>,
        "key_disagreements": ["point1", "point2"]
    }}
    """

    result = await arbiter_model.complete(arbiter_prompt)
    return ConvergenceResult(
        score=result['overall_convergence'],
        method="arbiter",
        details=result
    )
```

### Debate Round Orchestration

#### Round 0: Initial Research & Positions

```python
async def run_initial_round(model_a: str, model_b: str, context: DecisionContext):
    """Both models research and form initial positions"""

    # Parallel research with individual depth choice
    task_a = research_and_recommend(model_a, context)
    task_b = research_and_recommend(model_b, context)

    response_a, response_b = await asyncio.gather(task_a, task_b)

    # Models choose their own research depth
    # One doing 2 searches vs 5 shows different confidence levels
    research_score_a = len(response_a.tool_calls) * 0.2  # Depth bonus
    research_score_b = len(response_b.tool_calls) * 0.2

    return DebateRound(
        round_num=0,
        type="initial",
        response_a=response_a,
        response_b=response_b,
        metadata={
            'research_depth_a': len(response_a.tool_calls),
            'research_depth_b': len(response_b.tool_calls)
        }
    )
```

#### Challenge Rounds (Odd Numbers)

```python
async def run_challenge_round(
    round_num: int,
    model_a: str,
    model_b: str,
    prev_round: DebateRound,
    debate_history: List[DebateRound]
):
    """Each model challenges the other's position"""

    # Model A challenges B
    challenge_prompt_a = f"""
    You are in a debate about an architectural decision.

    Your opponent's current position:
    {prev_round.response_b.recommendation}

    Their sources and evidence:
    {format_sources(prev_round.response_b.sources)}

    Their confidence factors:
    {prev_round.response_b.confidence_factors}

    Provide a BRUTALLY HONEST review. You must:
    1. Identify at least 2 weaknesses or gaps
    2. Challenge questionable claims with evidence
    3. Propose superior alternatives if they exist
    4. Acknowledge any strong points (but don't be a yes-man)

    You may run up to 2 research queries to verify claims or find counter-evidence.

    Focus on their weak confidence areas: {prev_round.response_b.uncertainty_points}

    Remember: Agreement without substance scores poorly. Your rating improves by finding legitimate issues others missed.
    """

    # Model B challenges A (symmetric)
    challenge_prompt_b = create_challenge_prompt(model_b, prev_round.response_a)

    # Parallel execution with research budget
    task_a = model_challenge(model_a, challenge_prompt_a, max_tools=2)
    task_b = model_challenge(model_b, challenge_prompt_b, max_tools=2)

    response_a, response_b = await asyncio.gather(task_a, task_b)

    return DebateRound(
        round_num=round_num,
        type="challenge",
        response_a=response_a,
        response_b=response_b
    )
```

#### Defense Rounds (Even Numbers)

```python
async def run_defense_round(
    round_num: int,
    model_a: str,
    model_b: str,
    challenge_round: DebateRound,
    original_positions: DebateRound
):
    """Each model defends against challenges and potentially updates position"""

    defense_prompt_a = f"""
    Your architectural recommendation has been challenged.

    Your original position:
    {original_positions.response_a.recommendation}

    The challenges raised:
    {challenge_round.response_b.challenges}  # B's challenges to A

    Respond by:
    1. Defending valid points with evidence
    2. Acknowledging legitimate weaknesses
    3. Updating your recommendation if the challenges are valid
    4. Maintaining your position if the challenges are weak

    If you update your position, explain what changed and why.
    Your new confidence scores should reflect any doubts raised.

    You may run up to 2 research queries to support your defense.
    """

    defense_prompt_b = create_defense_prompt(model_b, challenge_round.response_a, original_positions.response_b)

    task_a = model_defend(model_a, defense_prompt_a, max_tools=2)
    task_b = model_defend(model_b, defense_prompt_b, max_tools=2)

    response_a, response_b = await asyncio.gather(task_a, task_b)

    return DebateRound(
        round_num=round_num,
        type="defense",
        response_a=response_a,
        response_b=response_b,
        position_changed_a=detect_position_change(original_positions.response_a, response_a),
        position_changed_b=detect_position_change(original_positions.response_b, response_b)
    )
```

### Tournament Progression with History

When a winner advances to face a new challenger:

```python
async def run_tournament_match(
    winner: str,
    winner_position: ModelResponse,
    challenger: str,
    match_history: List[DebateMatch],
    context: DecisionContext
):
    """Run match with tournament history context"""

    # Challenger gets to see previous debates
    challenger_prompt = f"""
    You are entering an architectural decision debate tournament.

    The current leader is {winner} with this position:
    {winner_position.recommendation}

    Previous debate highlights:
    {format_match_history(match_history)}

    Key points already discussed:
    - {extract_key_points(match_history)}

    You must provide a fresh perspective. Do not repeat arguments that have already been addressed.
    Focus on new angles, unconsidered trade-offs, or superior alternatives.

    Research the topic and form your initial position.
    Remember: You're not biased by the history - if the leader is wrong, prove it.
    """

    # Winner continues with refined position
    winner_prompt = f"""
    You've won the previous round with your recommendation.
    A new challenger is entering the debate.

    Briefly restate your position and await their challenge.
    Your position has been refined through debate:
    {winner_position.recommendation}

    Confidence: {winner_position.confidence}
    """

    # Run the match with history context
    return await run_match(winner, challenger, winner_prompt, challenger_prompt, match_history)
```

### Failure Modes & Edge Cases

#### Non-Convergent Debates

When models can't agree after max rounds:

```python
async def handle_non_convergence(match: DebateMatch) -> NonConvergentResult:
    """Handle debates that don't converge"""

    # Get arbiter's analysis of the fundamental disagreement
    analysis = await analyze_disagreement(match)

    # Present both positions clearly
    result = NonConvergentResult(
        model_a=match.model_a,
        model_b=match.model_b,
        position_a=match.final_position_a,
        position_b=match.final_position_b,
        fundamental_disagreement=analysis.core_disagreement,
        rounds_completed=len(match.rounds),
        convergence_scores=[r.convergence_score for r in match.rounds],
        arbiter_forced_choice=analysis.recommended_position,
        why_no_convergence=analysis.disagreement_reason
    )

    # Special UI treatment for non-convergent debates
    print(f"""
    ⚠️ Models could not reach consensus after {len(match.rounds)} rounds

    Fundamental disagreement: {analysis.core_disagreement}

    {match.model_a} maintains: {match.final_position_a.summary}
    {match.model_b} maintains: {match.final_position_b.summary}

    Convergence progression: {' → '.join(f"{s}%" for s in result.convergence_scores)}

    Arbiter's assessment: {analysis.recommended_position}

    View full debate with --show-debate to understand the disagreement.
    """)

    return result
```

#### Model Failures Mid-Debate

```python
async def handle_model_failure(
    match: DebateMatch,
    failed_model: str,
    round_num: int,
    error: Exception
) -> DebateResult:
    """Gracefully handle model failures during debate"""

    if round_num == 0:
        # Initial round failure - try backup model
        backup = await get_backup_model(failed_model)
        if backup:
            return await restart_match_with_backup(match, backup)
        else:
            # Can't proceed without initial positions
            raise DebateInitializationError(failed_model, error)

    elif round_num == 1:
        # Challenge round failure - other model wins by default
        winner = match.model_a if failed_model == match.model_b else match.model_b
        return DebateResult(
            winner=winner,
            reason="Opponent failed to provide challenge",
            rounds_completed=round_num
        )

    else:
        # Later round failure - judge based on current positions
        return await arbiter_judge_incomplete_debate(match, failed_model, round_num)
```

### Caching Debate Chains

```python
def get_debate_cache_key(
    decision_type: str,
    context: str,
    model_a: str,
    model_b: str,
    version: str
) -> str:
    """Generate deterministic cache key for debate chain"""

    # Order models consistently for cache hits
    models = sorted([model_a, model_b])

    components = [
        decision_type,
        normalize_context(context),
        models[0],
        models[1],
        version  # Prompt version to invalidate on changes
    ]

    return hashlib.sha256('\n'.join(components).encode()).hexdigest()

async def cache_debate_chain(match: DebateMatch):
    """Cache entire debate for replay"""

    cache_data = {
        'match': match.to_dict(),
        'rounds': [r.to_dict() for r in match.rounds],
        'convergence_scores': match.convergence_scores,
        'winner': match.winner,
        'total_cost': match.total_cost,
        'timestamp': datetime.utcnow()
    }

    await redis.setex(
        f"debate:{match.cache_key}",
        86400 * 7,  # 7 day TTL
        json.dumps(cache_data)
    )
```

## Consequences

### Positive

- **Rich debate dynamics** with clear progression
- **Graceful failure handling** at every stage
- **Full debate caching** for instant replay
- **Tournament context** prevents repetitive arguments
- **Clear convergence metrics** for transparency

### Negative

- **Complex orchestration** with many states
- **Potential for deadlock** if models are stubborn
- **Cache size** grows with debate chains

### Mitigation

- State machine pattern for clean orchestration
- Maximum rounds prevent infinite debates
- Compression for cached debate chains
- Arbiter override when convergence fails

---

**Stop Point**: Before Part 4 (Implementation Roadmap), please confirm:

1. The convergence measurement approach (3-layer: structural → semantic → arbiter) - good?
2. Should defense rounds allow models to completely change their position, or just refine?
3. The failure handling strategies - any other edge cases to consider?
4. Cache key strategy - should we include more context parameters?

Ready for Part 4 when you confirm!
